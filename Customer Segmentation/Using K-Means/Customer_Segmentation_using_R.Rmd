---
title: "Customer Segmentation using R"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

# Data Exploration

### Importing data from csv format

```{r}
customer_data <- read.csv("Mall_Customers.csv")
```

### Have a look at the dataset

```{r}
head(customer_data)
```
### Structure of the dataset

```{r}
str(customer_data)
```

### Columns of the dataset

```{r}
names(customer_data)
```

### Summary of the dataset

```{r}
summary(customer_data)
```

# Data Visualization and Analyzing Variables

### Customer Gender visualization
#### Creating a barplot and a piechart to show the gender distribution across the customer data

```{r}
gender_data <- table(customer_data$Gender)
barplot(gender_data, main = "Barplot of Customer Gender Comparison", ylab="Count", xlab="Gender", col=c("pink", "lightblue"), legend=rownames(gender_data))
```
```{r}
pct_gender=round(gender_data/sum(gender_data)*100)
lbs=paste(c("Female","Male")," ",pct_gender,"%")

library(plotrix)
pie3D(pct_gender,labels=lbs,main="Pie Chart Depicting the Ratio of Female and Male", col=c("pink", "lightblue"))
```

#### The barplot and pie chart concludes the customer dataset has more female customers at 56% ratio as compared to male customers at 44%


### Customer Age Distribution visualization

```{r}
summary(customer_data$Age)
sd(customer_data$Age)
```

#### Creating histogram to view the frequency of customer ages

```{r}
hist(customer_data$Age, 
     col="#92C5DE", 
     main="Histogram to Show Frequency of Age Class", 
     xlab="Age Class", ylab="Frequency", 
     labels=TRUE)
```
```{r}
boxplot(customer_data$Age, 
        col="#D6604D", 
        main="Boxplot for Descriptive Analysis of Age")
```

#### The above two visualizations conclude that the maximum customers are aged between 30 and 35, also the minimun and maximum age of the customers are 18 and 70 respectively.


### Analysis and Visualization of Annual Income of the Customers

```{r}
summary(customer_data$Annual.Income..k..)
sd(customer_data$Annual.Income..k..)
```
```{r}
hist(customer_data$Annual.Income..k..,
     col="#009999",
     main="Histogram for Annual Income",
     xlab="Annual Income Class",
     ylab="Frequency",
     labels=TRUE)
```
```{r}
plot(density(customer_data$Annual.Income..k..),
     main="Density Plot for Annual Income",
     xlab="Annual Income Class",
     ylab="Density")

polygon(density(customer_data$Annual.Income..k..),
        col="#AE123A")
```

#### The above two visualizations show that minimum and maximum annual incomes of our customers ranging from 15 and 137. The average income is 60.56. The Kernel Density Plot draws the normal distribution for the variable.


### Analysis and Visualization of Spending Score of the Customers

```{r}
summary(customer_data$Spending.Score..1.100.)
sd(customer_data$Spending.Score..1.100.)
```
```{r}
hist(customer_data$Spending.Score..1.100.,
     main="HistoGram for Spending Score",
     xlab="Spending Score Class",
     ylab="Frequency",
     col="#6DB562",
     labels=TRUE)
```

#### Minimum and maximum spending score of our customers are 1 and 99 respectively, as well the average score is 50.20. The above histogram shows the highest number of customers are having spending score between 40 and 50.


# K-means Algorithm

#### K-means algorithm is used for clustering, that computes the centroids and iterates until it finds the optimal centroid. The initial step is to randomly select k objects that are the means of the clusters. The remaining objects fall in the same cluster as their closest cluster mean, by finding Euclidean Distance between the objects. After each data object is assigned to a cluster, new mean value is calculated for each cluster in the data. New centers are recalculated and the observations are checked if they are closer to different clusters, if so the object is reassigned to a new cluster. This process is iterated several times until no new alterations in cluster assignments, that is the clusters in the new iteration are the same as obtained in the previous iteration.

# Optimal Cluster Selection

#### Below methods are popular while determining optimal number of clusters
#### 1. Elbow method
#### 2. Silhouette method
#### 3. Gap statistic

### Elbow method
```{r}
library(purrr)
```
```{r}
set.seed(123)
iss <- function(k){
  kmeans(customer_data[,3:5], k, iter.max=100, nstart=100, algorithm = "Lloyd")$tot.withinss
}

k.values <- 1:10

iss_values <- map_dbl(k.values, iss)

plot(k.values, iss_values, 
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K", 
     ylab = "Total intra-clusters sum of squares")
```

#### The elbow plot seems be appearing at the bend at 4, hence the appropriate number of clusters is 4.

### Average Silhouette Method
```{r}
library(cluster)
library(gridExtra)
library(grid)
```
```{r}
k2 <- kmeans(customer_data[,3:5], 2, iter.max = 100, nstart = 50, algorithm = "Lloyd")
s2 <- plot(silhouette(k2$cluster, dist(customer_data[,3:5], "euclidean")))
```

```{r}
k3 <- kmeans(customer_data[,3:5], 3, iter.max = 100, nstart = 50, algorithm = "Lloyd")
s3 <- plot(silhouette(k3$cluster, dist(customer_data[,3:5], "euclidean")))
```
```{r}
k4 <- kmeans(customer_data[,3:5], 4, iter.max = 100, nstart = 50, algorithm = "Lloyd")
s4 <- plot(silhouette(k4$cluster, dist(customer_data[,3:5], "euclidean")))
```

```{r}
k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))
```


#### Now, we make use of the fviz_nbclust() function to determine and visualize the optimal number of clusters as follows
```{r}
library(NbClust)
library(factoextra)
```


```{r}
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")
```
```{r}
set.seed(125)
stat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,
            K.max = 10, B = 50)
fviz_gap_stat(stat_gap)
```
```{r}
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6
```
```{r}
pcclust=prcomp(customer_data[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)

pcclust$rotation[,1:2]
```
```{r}
set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```
```{r}
ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```
```{r}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))
```


